{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Classifying Amazon Healthcare Product Reviews (Revised With the Addition of A Neural Network)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Sentiment analysis is a technique that uses machine learning and mathematical models to quantitatively analyze subjective information such as text. Due to an exponentially increasing amount of data being generated, companies rely on machine learning techniques such as sentiment analysis to gather insights from this data that can help them make impactful decisions. Sentiment analysis has many applications in product management such as customer sentiment analysis. By being able to understand information gained from customer feedback, decisionmakers can efficiently create strategies to improve product performance. To better understand how product feedback can be utilized, I am interested in using sentiment analysis to analyze customer reviews on healthcare products sold on Amazon.\n",
    "\n",
    "In this study, sentiment analysis was done on healthcare product reviews using several types of supervised learning classification models. Models focused exclusively on utilizing the review text data to better gauge the impact of the sentiment analysis techniques. The different models were compared to better understand their ability to analyze the product review data. The process used to undertake this study is as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "Data Exploration and Analysis \n",
    "* Analyzing the Sources of the Text Information\n",
    "* Understanding the Sentiment of the Reviews Based on the Rating\n",
    "* Viewing the Distribution of the Overall Rating\n",
    "\n",
    "Preparing The Data For Modeling\n",
    "* Labeling the Reviews Based on Rating\n",
    "* Vectorizing the Text Data\n",
    "* Dealing With Class Imbalance\n",
    "* Feature Selection\n",
    "\n",
    "Modeling the Data \n",
    "* Naive Bayes\n",
    "* K Nearest Neighbors\n",
    "* Decision Trees\n",
    "* Random Forest\n",
    "* Logistic Regression (and Lasso and Ridge)\n",
    "* Support Vector Classifier\n",
    "* Gradient Boost\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 546 ms, total: 2.13 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import math\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib.mlab import PCA as mlabPCA\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import neighbors\n",
    "from sklearn.utils import resample\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"sklearn\"  \n",
    "    )\n",
    "\n",
    "# Set Plot Style\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 993 ms, total: 5.48 s\n",
      "Wall time: 5.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Import Files\n",
    "\n",
    "df = pd.read_json('Health_and_Personal_Care_5.json', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View Data for Cleaning\n",
    "\n",
    "#df.head(7)\n",
    "#df.dtypes\n",
    "#df.describe()\n",
    "#df.isnull().sum(axis = 0)\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 107 ms, sys: 6.39 ms, total: 114 ms\n",
      "Wall time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Drop Unnecessary Columns\n",
    "\n",
    "df = df.drop(['reviewerName', 'reviewTime'] , axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 s, sys: 105 ms, total: 2.91 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Converting Helpfulness Rating to float\n",
    "\n",
    "df.helpful = df.helpful.apply(lambda x: str(x).replace('[','').replace(', ','/').replace(']',''))\n",
    "df.helpful = df.helpful.replace('0/0','0')\n",
    "df.helpful = df.helpful.apply(eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Analysis\n",
    "\n",
    "Dataset used for this study includes product reviews and information about the context of the reviews.\n",
    "Such information includes: the time the reviews were posted, the product ID, a helpfulness rating, a review summary, and an overall rating of the products.\n",
    "The data being used in this study comes from the Amazon website and reflect data was collected between 1996 and 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great little gadget to have around. ...</td>\n",
       "      <td>ALC5GH8CAMAI7</td>\n",
       "      <td>Handy little gadget</td>\n",
       "      <td>1294185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4</td>\n",
       "      <td>I would recommend this for a travel magnifier ...</td>\n",
       "      <td>AHKSURW85PJUE</td>\n",
       "      <td>Small &amp; may need to encourage battery</td>\n",
       "      <td>1329523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>0.974</td>\n",
       "      <td>4</td>\n",
       "      <td>What I liked was the quality of the lens and t...</td>\n",
       "      <td>A38RMU1Y5TDP9</td>\n",
       "      <td>Very good but not great</td>\n",
       "      <td>1275955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>0.933</td>\n",
       "      <td>4</td>\n",
       "      <td>Love the Great point light pocket magnifier!  ...</td>\n",
       "      <td>A1XZUG7DFXXOS4</td>\n",
       "      <td>great addition to your purse</td>\n",
       "      <td>1202428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5</td>\n",
       "      <td>This is very nice. You pull out on the magnifi...</td>\n",
       "      <td>A1MS3M7M7AM13X</td>\n",
       "      <td>Very nice and convenient.</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>0.667</td>\n",
       "      <td>5</td>\n",
       "      <td>The light comes on when the item is pulled.  T...</td>\n",
       "      <td>AXO4PQU0XG3TG</td>\n",
       "      <td>$9.99, pretty and cute</td>\n",
       "      <td>1172275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>159985130X</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "      <td>These are lightweight and efficient and have s...</td>\n",
       "      <td>A28X0LT2100RL1</td>\n",
       "      <td>Lightweight and efficient</td>\n",
       "      <td>1404604800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  helpful  overall  \\\n",
       "0  159985130X    1.000        5   \n",
       "1  159985130X    1.000        4   \n",
       "2  159985130X    0.974        4   \n",
       "3  159985130X    0.933        4   \n",
       "4  159985130X    1.000        5   \n",
       "5  159985130X    0.667        5   \n",
       "6  159985130X    0.000        4   \n",
       "\n",
       "                                          reviewText      reviewerID  \\\n",
       "0  This is a great little gadget to have around. ...   ALC5GH8CAMAI7   \n",
       "1  I would recommend this for a travel magnifier ...   AHKSURW85PJUE   \n",
       "2  What I liked was the quality of the lens and t...   A38RMU1Y5TDP9   \n",
       "3  Love the Great point light pocket magnifier!  ...  A1XZUG7DFXXOS4   \n",
       "4  This is very nice. You pull out on the magnifi...  A1MS3M7M7AM13X   \n",
       "5  The light comes on when the item is pulled.  T...   AXO4PQU0XG3TG   \n",
       "6  These are lightweight and efficient and have s...  A28X0LT2100RL1   \n",
       "\n",
       "                                 summary  unixReviewTime  \n",
       "0                    Handy little gadget      1294185600  \n",
       "1  Small & may need to encourage battery      1329523200  \n",
       "2                Very good but not great      1275955200  \n",
       "3           great addition to your purse      1202428800  \n",
       "4              Very nice and convenient.      1313452800  \n",
       "5                 $9.99, pretty and cute      1172275200  \n",
       "6              Lightweight and efficient      1404604800  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18534"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.asin.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were reviews for 18534 different products included in this study. This means that there was an average of about 19 reviews per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38609"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.reviewerID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were reviews from 38609 different reviewers included in this study. This means that there was an average of about 9 reviews per reviewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of a user who gave the product a 5 rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is very nice. You pull out on the magnifier when you want the light to come on, then slide it back in. I would recommend buying this if you need something with a light that you can easily put in your pocket or purse.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.query('overall == 5').reviewText)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of a user who gave the product a 4 rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What I liked was the quality of the lens and the built in light.  Then lens had no discernable distortion anywhere.  It magnified everything evenly without the ripples and  distortion that I've seen with other low cost magnifiers.  This light is a nice touch and easy to use.  If you want it on just pull the lens out a bit.  It is focused very close to the center of what you will be look at and provides nice, even coverage.What I didn't like was the brightness (actually dimmness) of the light and where it is focused.  LEDs can be lots brighter, I know as I've seen them.  Also, the light focuses at the center of you field of view but only when the lens is too close to be focused properly.Bottom line is this is a good value for a magnifier and could have been made great with better quality control.BTW, I feel that honest, effective reviews can take the place of first-hand experiences that are lacking in online shopping. I've always appreciated the help I've received from other reviewers and work hard to return the favor as best as I can.  I hope you found this review helpful and if there was anything you thought was lacking or unclear leave a comment and I'll do what I can to fix it.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.query('overall == 4').reviewText)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of a user who gave the product a 3 rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This magnifier has nothing to cover it when not in use.I compared it with a Carson 3x magnifier and this one did not seem as clear as the Carson - hard to see the furigana clearly in Japanese comic books with this one.The Carson ones come with a cover.I would recommend the Carson 5x if you are looking for good size enlarging.Carson MiniBrite 5x Power Slide- Out MagnifierI wish I had skipped the 3x and gone with the 5x only.For this one in the 3x, the light works well and lights things up nicely.Carson MiniBrite 5x Power Slide- Out Magnifier'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.query('overall == 3').reviewText)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of a user who gave the product a 2 rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bought for my mother due her eye sight going downhill. She said she still can not read what she wants to she said her little magnifier that has a light on it is better. She said she would not recommend.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.query('overall == 2').reviewText)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of a user who gave the product a 1 rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ONE STAR:The Maxell LR44 10-pack photo shows the new hologram packaging, but I received the old orange & black packaging.The batteries are stale. Lights powered by them are semi-bright, and only last a day or so.The orange & black pack rates 1-star.FIVE STARS:From the same supplier, MyBatterySupplier, I ordered the50-pack, which did come in the new hologram package, and the difference was dramatic.  Lights powered by the batteries were brilliant, and I expect them to last much longer.The new hologram pack rates 5-stars.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.query('overall == 1').reviewText)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a large shift in sentiment from positive to negative as product ratings change from being 4 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    211633\n",
       "4     68168\n",
       "3     33254\n",
       "2     16754\n",
       "1     16546\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.overall.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of reviews skew greatly toward a 5 rating. Since reviews will be labeled based on the ratings, class balancing will need to be done to reduce the impact of the label that will reflect the 5 ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing The Data For Modeling\n",
    "\n",
    "To prepare the data for modeling, the review text data was isolated and a new feature was engineered engineered to label the reviews as positive or negative. The review data was vectorized and the number of features was reduced using SVD and the selectKbest function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Outcome Variable (will move to prep stage after analysis)\n",
    "\n",
    "df['sentiment'] = np.where(df['overall'] > 3, 1, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews accompanied by a rating of 4 or higher were labeled as positive (1) and reviews of 3 and below were labeled as negative or (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    279801\n",
       "0     66554\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking for Class Imbalance in the Outcome Class\n",
    "\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive reviews make a majority of the reviews while the negative reviews make the minority. Class balancing will be done so that the models won't indiscriminately predict the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate Original Dataset into Separate DataFrames for Different Cleaning Implementations\n",
    "\n",
    "df_without_text = df.drop(['reviewText', 'summary', 'reviewerID', 'asin'], axis=1)\n",
    "df_review_text = df.reviewText\n",
    "df_review_summary = df.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 23s, sys: 7.97 s, total: 11min 31s\n",
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Vectorizing Text Data\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=stemming_tokenizer, max_features=1000, use_idf=True)\n",
    "X = tfidf_vectorizer.fit_transform(df_review_text)\n",
    "df_review_text_vectorized = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=stemming_tokenizer, max_features=1000, use_idf=True)\n",
    "X = tfidf_vectorizer.fit_transform(df_review_summary)\n",
    "df_review_summary_vectorized = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "## Drop Unnecessary Columns\n",
    "\n",
    "df_review_text_vectorized = df_review_text_vectorized.drop(df_review_text_vectorized.columns[:26], axis=1)\n",
    "df_review_summary_vectorized = df_review_summary_vectorized.drop(df_review_summary_vectorized.columns[:18], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 32.763003479931506\n",
      "CPU times: user 1min 38s, sys: 9.42 s, total: 1min 48s\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Dimension reduction of Review Text DataFrame\n",
    "\n",
    "svd= TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "df_review_text_components = lsa.fit_transform(df_review_text_vectorized)\n",
    "\n",
    "df_review_text_components = pd.DataFrame(df_review_text_components)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 401 ms, sys: 114 ms, total: 516 ms\n",
      "Wall time: 538 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Creating DataFrame For Modeling\n",
    "\n",
    "df = pd.concat([df.sentiment, df_review_text_components], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    150000\n",
      "0    150000\n",
      "Name: sentiment, dtype: int64\n",
      "CPU times: user 406 ms, sys: 261 ms, total: 667 ms\n",
      "Wall time: 694 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Class Balancing\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.sentiment==1]\n",
    "df_minority = df[df.sentiment==0]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=150000,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=150000,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    "\n",
    "print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class balancing was done by downsampling the majority class and upsampling the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.8 ms, sys: 67.4 ms, total: 154 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Establish variables based on original features to be used for modeling\n",
    "\n",
    "x = df.drop(['sentiment'], axis=1)\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 417 ms, sys: 233 ms, total: 650 ms\n",
      "Wall time: 662 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Establish variables based on select K best to be used for modeling\n",
    "\n",
    "selector = SelectKBest(f_classif, k=50)\n",
    "k_predictors = selector.fit_transform(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 32 ms, total: 178 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Train Test Split Original Variables And K Selected Variables for Modeling\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(k_predictors, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x and y variables represent the variables to be used for training and testing the different supervised learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 ms, sys: 56.2 ms, total: 384 ms\n",
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## train and fit model\n",
    "\n",
    "bnb = BernoulliNB().fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.66785\n",
      "\n",
      "cross validation:\n",
      "[0.66975    0.6705     0.67233333 0.66341667 0.67483333]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.73179936 0.7283914  0.73166425 0.72167188 0.7349367 ]\n",
      "\n",
      "confusion matrix:\n",
      "[[19852  9953]\n",
      " [ 9976 20219]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67     29805\n",
      "           1       0.67      0.67      0.67     30195\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     60000\n",
      "   macro avg       0.67      0.67      0.67     60000\n",
      "weighted avg       0.67      0.67      0.67     60000\n",
      "\n",
      "CPU times: user 3.71 s, sys: 628 ms, total: 4.33 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(bnb.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(bnb, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(bnb, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, bnb.predict(x_test)))+'\\n')\n",
    "\n",
    "print(classification_report(y_test, bnb.predict(x_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes had relatively low accuracy compared to most of the other models.\n",
    "The cross validation showed that overfitting was not greatly present with this model. \n",
    "The model was slightly better at predicting the positive class.\n",
    "The model assumes that the variables are uncorrelated, which is true because they have been reduced to svd components. However this also means that the model's performance may have been negatively impacted by its inability to capture the combined effect of multiple variables on the outcome. For example a word like 'good' may have a positive connotation, but another word like 'not' could change the context of 'good' to have a negative meaning ('not good'). Naive bayes would fail to capture the combined meaning of the two words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 s, sys: 33.6 ms, total: 4.12 s\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## train and fit model\n",
    "\n",
    "decision_tree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features=6,\n",
    "    max_depth=25,\n",
    "    ).fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.8057333333333333\n",
      "\n",
      "cross validation:\n",
      "[0.648      0.6555     0.65475    0.65008333 0.64966667]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.65708832 0.65760371 0.65757921 0.66166587 0.65967743]\n",
      "\n",
      "confusion matrix:\n",
      "[[26571  3234]\n",
      " [ 8422 21773]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82     29805\n",
      "           1       0.87      0.72      0.79     30195\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     60000\n",
      "   macro avg       0.81      0.81      0.80     60000\n",
      "weighted avg       0.82      0.81      0.80     60000\n",
      "\n",
      "CPU times: user 7.56 s, sys: 119 ms, total: 7.68 s\n",
      "Wall time: 7.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(decision_tree.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(decision_tree, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(decision_tree, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, decision_tree.predict(x_test)))+'\\n')\n",
    "\n",
    "print(classification_report(y_test, decision_tree.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree had high accuracy compared to the other model types in this study.\n",
    "The cross validation showed that overfitting was not greatly present with this model. \n",
    "The model was better at predicting the negative class.\n",
    "The model's reliance on binary divisions likely improved its ability to capture nuance within the text and allows for greater discernment between reviews with correlated words and phrases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 125 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Fit and Train Model\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features=15,\n",
    "    max_depth=100,\n",
    "    ).fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.8415\n",
      "\n",
      "cross validation:\n",
      "[0.70925    0.706      0.70208333 0.69708333 0.70225   ]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.79241176 0.79025421 0.79212652 0.78454937 0.79060043]\n",
      "\n",
      "confusion matrix:\n",
      "[[27548  2257]\n",
      " [ 7253 22942]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85     29805\n",
      "           1       0.91      0.76      0.83     30195\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     60000\n",
      "   macro avg       0.85      0.84      0.84     60000\n",
      "weighted avg       0.85      0.84      0.84     60000\n",
      "\n",
      "CPU times: user 1min 46s, sys: 651 ms, total: 1min 47s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(rfc.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(rfc, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(rfc, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, rfc.predict(x_test)))+'\\n')\n",
    "\n",
    "print(classification_report(y_test, rfc.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest had relatively high accuracy compared to most of the other models.\n",
    "The cross validation showed that overfitting had very little prescence with this model. \n",
    "The model was better at predicting the negative outcome class.\n",
    "The model's success likely comes from it not having to rely on each set of data being evaluated only once. \n",
    "By being able to base its evaluations on multiple sub decision trees classifications are only finalized after multiple iterations.\n",
    "By building on the ability of the decision tree to divide the data based on varying contextual information, the random forest model was able to sustain relatively high performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 135 ms, total: 1.94 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## train and fit model\n",
    "\n",
    "lr = LogisticRegression(fit_intercept=False).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.7146\n",
      "\n",
      "cross validation:\n",
      "[0.71716667 0.71066667 0.71458333 0.71016667 0.71908333]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.79167359 0.78817354 0.79008466 0.78647806 0.79518103]\n",
      "\n",
      "confusion matrix:\n",
      "[[21379  8426]\n",
      " [ 8698 21497]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71     29805\n",
      "           1       0.72      0.71      0.72     30195\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     60000\n",
      "   macro avg       0.71      0.71      0.71     60000\n",
      "weighted avg       0.71      0.71      0.71     60000\n",
      "\n",
      "CPU times: user 6.31 s, sys: 1.16 s, total: 7.47 s\n",
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(lr.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(lr, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(lr, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, lr.predict(x_test)))+'\\n')\n",
    "\n",
    "print(classification_report(y_test, lr.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The logistic regression models had middling accuracy compared to most of the other models.\n",
    "The cross validation showed that overfitting was not greatly present with this model. \n",
    "The model had similar rates of type 1 and type 2 error.\n",
    "The model likely benefitted from being able to reduce the impact of parameters that were deemed to be excessively low.\n",
    "This quality is especially useful when modeling text data because there's a large amount of words that wouldn't be used often enough to have a meaningful impact on the model, and words where there isn't a strong positive or negative connotation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 47s, sys: 10.4 s, total: 1h 13min 57s\n",
      "Wall time: 1h 14min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## train and fit model\n",
    "\n",
    "svc = SVC().fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.7168666666666667\n",
      "\n",
      "cross validation:\n",
      "[0.71941667 0.71308333 0.71408333 0.71175    0.71925   ]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.79124737 0.78816637 0.78936463 0.78624505 0.79545159]\n",
      "\n",
      "confusion matrix:\n",
      "[[21957  7848]\n",
      " [ 9140 21055]]\n",
      "\n",
      "CPU times: user 47min 50s, sys: 8.24 s, total: 47min 59s\n",
      "Wall time: 48min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(svc.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(svc, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(svc, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, svc.predict(x_test)))+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72     29805\n",
      "           1       0.73      0.70      0.71     30195\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     60000\n",
      "   macro avg       0.72      0.72      0.72     60000\n",
      "weighted avg       0.72      0.72      0.72     60000\n",
      "\n",
      "CPU times: user 9min 16s, sys: 1.59 s, total: 9min 18s\n",
      "Wall time: 9min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Classification Report\n",
    "\n",
    "print(classification_report(y_test, svc.predict(x_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier had middling accuracy compared to most of the other models.\n",
    "The cross validation showed that overfitting had very little prescence in in this model. \n",
    "The model was marginally better at predicting the negative class.\n",
    "The model relies on creating boundaries between datapoints that reflect different classes. \n",
    "With text data, those boundaries are be harder to form due to the existence of words that show up a lot in poth positive and negative reviews. \n",
    "As a result, the model relies more on its cost function to reduce error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'deviance', 'max_depth': 2, 'n_estimators': 300}\n",
      "CPU times: user 1h 29min 25s, sys: 32.6 s, total: 1h 29min 58s\n",
      "Wall time: 1h 30min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## train and fit model\n",
    "\n",
    "cl = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "parameters = { \n",
    "              'n_estimators': list(np.arange(200, 301, 50)),\n",
    "              'max_depth': list(range(1,3)),\n",
    "              'loss': ['deviance', 'exponential']\n",
    "             }\n",
    "\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "clf = GridSearchCV(cl, parameters, scoring=acc_scorer).fit(x_train,  y_train)\n",
    "\n",
    "## Show Best Parameters\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.71485\n",
      "\n",
      "cross validation:\n",
      "[0.71125    0.70925    0.71441667 0.706      0.7135    ]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.78811327 0.78279881 0.78677194 0.78261945 0.79045135]\n",
      "\n",
      "confusion matrix:\n",
      "[[21886  7919]\n",
      " [ 9190 21005]]\n",
      "\n",
      "CPU times: user 2h 29min 35s, sys: 58.2 s, total: 2h 30min 33s\n",
      "Wall time: 2h 30min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(clf.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(clf, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(clf, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, clf.predict(x_test)))+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72     29805\n",
      "           1       0.73      0.70      0.71     30195\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     60000\n",
      "   macro avg       0.72      0.71      0.71     60000\n",
      "weighted avg       0.72      0.71      0.71     60000\n",
      "\n",
      "CPU times: user 302 ms, sys: 5.64 ms, total: 307 ms\n",
      "Wall time: 308 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Classification Report\n",
    "\n",
    "print(classification_report(y_test, clf.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boost model had middling accuracy compared to the other models.\n",
    "The cross validation showed few signs of overfitting with this model. \n",
    "The model was equally good at predicting both classes.\n",
    "The strength of this model when it comes to making predictions using this data comes from its ability to reduce error over multiple iterations, while building on the strengths of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 50s, sys: 2min 4s, total: 21min 54s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## train and fit model\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,)).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.7371\n",
      "\n",
      "cross validation:\n",
      "[0.72091667 0.71683333 0.717      0.71675    0.715     ]\n",
      "\n",
      "cross validation with AUC:\n",
      "[0.79784296 0.79327906 0.79480397 0.79082147 0.79362804]\n",
      "\n",
      "confusion matrix:\n",
      "[[21792  8013]\n",
      " [ 7761 22434]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.73     29805\n",
      "           1       0.74      0.74      0.74     30195\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     60000\n",
      "   macro avg       0.74      0.74      0.74     60000\n",
      "weighted avg       0.74      0.74      0.74     60000\n",
      "\n",
      "CPU times: user 36min 33s, sys: 4min 23s, total: 40min 57s\n",
      "Wall time: 10min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "print(\"accuracy score:\\n\" + str(mlp.score(x_test, y_test))+'\\n')\n",
    "\n",
    "print(\"cross validation:\\n\" + str(cross_val_score(mlp, x_test, y_test, cv=5))+'\\n')\n",
    "\n",
    "print(\"cross validation with AUC:\\n\" + str(cross_val_score(mlp, x_test, y_test, cv=5, scoring='roc_auc'))+'\\n')\n",
    "\n",
    "print(\"confusion matrix:\\n\" + str(confusion_matrix(y_test, mlp.predict(x_test)))+'\\n')\n",
    "\n",
    "print(classification_report(y_test, mlp.predict(x_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network had high accuracy compared to the other models.\n",
    "The cross validation showed few signs of overfitting with this model. \n",
    "The model was equally good at predicting both classes.\n",
    "This model outperformed the gradient boosting classifier and had significantly lower runtimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Conclusion\n",
    "\n",
    "The random forest was by far the best model when it came to performing sentiment analysis on the customer reviews.\n",
    "It is also important to note that the decision tree was the next best performing model, implying that the text data benefits greatly from the binary splitting processes that the decision tree undergoes.\n",
    "\n",
    "Understanding how to better utilize supervised modeling techniques to perform customer sentiment analysis will give insight how to understand feedback on products. \n",
    "Being able to act on these efficiently gathered insights, could result in strategic decisionmaking that can increase product quality.\n",
    "\n",
    "This study established the best suprvised modeling technique for determining the sentiment of Amazon Reviews. The next step in using this data to gather insights from reviews would be to collect review data from different sources and use them to test the model. This would give insight as to how different types of text are percieved by the model. By being able to understand how supervised learning models can be affected by text from different contexts, the increased efficiency of sentiment analysis could result in more nuanced insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
